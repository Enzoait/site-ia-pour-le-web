{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Using cached absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Using cached grpcio-1.73.1-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Using cached markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from tensorboard) (25.0)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Using cached protobuf-6.31.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from tensorboard) (65.5.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from tensorboard) (1.17.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\enzoa\\onedrive\\documents\\iim\\ia pour le web\\repo\\site-ia-pour-le-web\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.3.0 grpcio-1.73.1 markdown-3.8.2 protobuf-6.31.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib\n",
    "%pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Learn the Basics](intro.html) \\|\\| **Quickstart** \\|\\|\n",
    "[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n",
    "DataLoaders](data_tutorial.html) \\|\\|\n",
    "[Transforms](transforms_tutorial.html) \\|\\| [Build\n",
    "Model](buildmodel_tutorial.html) \\|\\|\n",
    "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
    "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
    "Model](saveloadrun_tutorial.html)\n",
    "\n",
    "Quickstart\n",
    "==========\n",
    "\n",
    "This section runs through the API for common tasks in machine learning.\n",
    "Refer to the links in each section to dive deeper.\n",
    "\n",
    "Working with data\n",
    "-----------------\n",
    "\n",
    "PyTorch has two [primitives to work with\n",
    "data](https://pytorch.org/docs/stable/data.html):\n",
    "`torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset`\n",
    "stores the samples and their corresponding labels, and `DataLoader`\n",
    "wraps an iterable around the `Dataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offers domain-specific libraries such as\n",
    "[TorchText](https://pytorch.org/text/stable/index.html),\n",
    "[TorchVision](https://pytorch.org/vision/stable/index.html), and\n",
    "[TorchAudio](https://pytorch.org/audio/stable/index.html), all of which\n",
    "include datasets. For this tutorial, we will be using a TorchVision\n",
    "dataset.\n",
    "\n",
    "The `torchvision.datasets` module contains `Dataset` objects for many\n",
    "real-world vision data like CIFAR, COCO ([full list\n",
    "here](https://pytorch.org/vision/stable/datasets.html)). In this\n",
    "tutorial, we use the FashionMNIST dataset. Every TorchVision `Dataset`\n",
    "includes two arguments: `transform` and `target_transform` to modify the\n",
    "samples and labels respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the `Dataset` as an argument to `DataLoader`. This wraps an\n",
    "iterable over our dataset, and supports automatic batching, sampling,\n",
    "shuffling and multiprocess data loading. Here we define a batch size of\n",
    "64, i.e. each element in the dataloader iterable will return a batch of\n",
    "64 features and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about [loading data in PyTorch](data_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Models\n",
    "===============\n",
    "\n",
    "To define a neural network in PyTorch, we create a class that inherits\n",
    "from\n",
    "[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "We define the layers of the network in the `__init__` function and\n",
    "specify how data will pass through the network in the `forward`\n",
    "function. To accelerate operations in the neural network, we move it to\n",
    "the\n",
    "[accelerator](https://pytorch.org/docs/stable/torch.html#accelerators)\n",
    "such as CUDA, MPS, MTIA, or XPU. If the current accelerator is\n",
    "available, we will use it. Otherwise, we use the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about [building neural networks in\n",
    "PyTorch](buildmodel_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing the Model Parameters\n",
    "===============================\n",
    "\n",
    "To train a model, we need a [loss\n",
    "function](https://pytorch.org/docs/stable/nn.html#loss-functions) and an\n",
    "[optimizer](https://pytorch.org/docs/stable/optim.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training\n",
    "dataset (fed to it in batches), and backpropagates the prediction error\n",
    "to adjust the model\\'s parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check the model\\'s performance against the test dataset to\n",
    "ensure it is learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (*epochs*).\n",
    "During each epoch, the model learns parameters to make better\n",
    "predictions. We print the model\\'s accuracy and loss at each epoch;\n",
    "we\\'d like to see the accuracy increase and the loss decrease with every\n",
    "epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.294829  [   64/60000]\n",
      "loss: 2.289838  [ 6464/60000]\n",
      "loss: 2.294821  [12864/60000]\n",
      "loss: 2.280479  [19264/60000]\n",
      "loss: 2.280589  [25664/60000]\n",
      "loss: 2.273757  [32064/60000]\n",
      "loss: 2.262761  [38464/60000]\n",
      "loss: 2.273851  [44864/60000]\n",
      "loss: 2.256245  [51264/60000]\n",
      "loss: 2.245424  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.2%, Avg loss: 2.247857 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.245859  [   64/60000]\n",
      "loss: 2.237273  [ 6464/60000]\n",
      "loss: 2.248697  [12864/60000]\n",
      "loss: 2.215683  [19264/60000]\n",
      "loss: 2.229772  [25664/60000]\n",
      "loss: 2.220819  [32064/60000]\n",
      "loss: 2.193219  [38464/60000]\n",
      "loss: 2.223562  [44864/60000]\n",
      "loss: 2.187671  [51264/60000]\n",
      "loss: 2.168211  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 2.175524 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.173925  [   64/60000]\n",
      "loss: 2.157932  [ 6464/60000]\n",
      "loss: 2.178286  [12864/60000]\n",
      "loss: 2.116082  [19264/60000]\n",
      "loss: 2.146621  [25664/60000]\n",
      "loss: 2.133353  [32064/60000]\n",
      "loss: 2.082172  [38464/60000]\n",
      "loss: 2.137140  [44864/60000]\n",
      "loss: 2.073169  [51264/60000]\n",
      "loss: 2.042549  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 2.052355 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.051195  [   64/60000]\n",
      "loss: 2.021832  [ 6464/60000]\n",
      "loss: 2.057497  [12864/60000]\n",
      "loss: 1.949492  [19264/60000]\n",
      "loss: 1.997181  [25664/60000]\n",
      "loss: 1.976972  [32064/60000]\n",
      "loss: 1.897031  [38464/60000]\n",
      "loss: 1.983441  [44864/60000]\n",
      "loss: 1.880167  [51264/60000]\n",
      "loss: 1.837915  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.843394 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.847208  [   64/60000]\n",
      "loss: 1.794523  [ 6464/60000]\n",
      "loss: 1.849770  [12864/60000]\n",
      "loss: 1.689718  [19264/60000]\n",
      "loss: 1.745506  [25664/60000]\n",
      "loss: 1.716774  [32064/60000]\n",
      "loss: 1.616769  [38464/60000]\n",
      "loss: 1.740209  [44864/60000]\n",
      "loss: 1.597332  [51264/60000]\n",
      "loss: 1.548880  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 1.543072 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about [Training your model](optimization_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Models\n",
    "=============\n",
    "\n",
    "A common way to save a model is to serialize the internal state\n",
    "dictionary (containing the model parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Models\n",
    "==============\n",
    "\n",
    "The process for loading a model includes re-creating the model structure\n",
    "and loading the state dictionary into it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"8\", Actual: \"8\"\n",
      "Accuracy: 100.0%\n",
      "Loss : 1.2072\n",
      "Predicted: \"3\", Actual: \"3\"\n",
      "Accuracy: 100.0%\n",
      "Loss : 1.5821\n",
      "Predicted: \"2\", Actual: \"2\"\n",
      "Accuracy: 100.0%\n",
      "Loss : 1.2325\n",
      "Predicted: \"1\", Actual: \"1\"\n",
      "Accuracy: 100.0%\n",
      "Loss : 0.7356\n",
      "Predicted: \"5\", Actual: \"5\"\n",
      "Accuracy: 100.0%\n",
      "Loss : 1.8032\n",
      "Predicted: \"2\", Actual: \"2\"\n",
      "Accuracy: 100.0%\n",
      "Loss : 1.0735\n",
      "Predicted: \"10\", Actual: \"5\"\n",
      "Accuracy: 0.0%\n",
      "Loss : 1.8307\n",
      "Predicted: \"10\", Actual: \"10\"\n",
      "Accuracy: 100.0%\n",
      "Loss : 1.8390\n",
      "Predicted: \"7\", Actual: \"6\"\n",
      "Accuracy: 0.0%\n",
      "Loss : 2.4666\n",
      "Predicted: \"8\", Actual: \"10\"\n",
      "Accuracy: 0.0%\n",
      "Loss : 1.4996\n",
      "Pour visualiser dans TensorBoard, lancez :\n",
      "  tensorboard --logdir=runs\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\",\n",
    "    \"10\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "nombre_de_tests = 10\n",
    "\n",
    "for i in range(0, nombre_de_tests):\n",
    "    x, y = test_data[i][0], test_data[i][1]\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        pred = model(x)\n",
    "        predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "        print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "        writer = SummaryWriter()\n",
    "\n",
    "        accuracy = (pred[0].argmax(0) == y).type(torch.float).item()\n",
    "        loss = loss_fn(pred, torch.tensor([y], device=device)).item()\n",
    "\n",
    "        writer.add_scalar('Sample/Accuracy', accuracy, i)\n",
    "        writer.add_scalar('Sample/Loss', loss, i)\n",
    "        writer.add_text('Sample/Prediction', f'Predicted: \"{predicted}\", Actual: \"{actual}\"', i)\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "        print(f'Accuracy: {(100 * accuracy):>0.1f}%')\n",
    "        print(f'Loss : {loss:>0.4f}')\n",
    "\n",
    "        writer = SummaryWriter()\n",
    "        writer.add_scalar('Sample/Accuracy', accuracy, i)\n",
    "        writer.add_scalar('Sample/Loss', loss, i)\n",
    "        writer.add_text('Sample/Prediction', f'Predicted: \"{predicted}\", Actual: \"{actual}\"', i)\n",
    "        writer.close()\n",
    "print(\"Pour visualiser dans TensorBoard, lancez :\\n  tensorboard --logdir=runs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"0\",\n",
    "    1: \"1\",\n",
    "    2: \"2\",\n",
    "    3: \"3\",\n",
    "    4: \"4\",\n",
    "    5: \"5\",\n",
    "    6: \"6\",\n",
    "    7: \"7\",\n",
    "    8: \"8\",\n",
    "    9: \"9\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about [Saving & Loading your\n",
    "model](saveloadrun_tutorial.html).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
